# 磁盘性能
## 影响硬盘性能的因素
```md
影响磁盘的关键因素是磁盘服务时间，即磁盘完成一个I/O请求所花费的时间，它由寻道时间、旋转延迟和数据传输时间三部分构成。
```
* 寻道时间
```md
指将读写磁头移动至正确的磁道上所需要的时间。寻道时间越短，I/O操作越快，目前磁盘的平均寻道时间一般在3-15ms。
```
* 旋转延迟
```md
指盘片旋转将请求数据所在扇区移至读写磁头下方所需要的时间。
旋转延迟取决于磁盘转速，通常使用磁盘旋转一周所需时间的1/2表示。
比如，7200 rpm的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms，而转速为15000 rpm的磁盘其平均旋转延迟约为2ms。
```
* 数据传输时间
```md
指完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据大小除以数据传输率。
IDE/ATA能达到133MB/s，SATA II可达到300MB/s的接口数据传输率。
数据传输时间通常远小于寻道时间和旋转延迟。简单计算时可忽略。
```

## 性能的指标
### IOPS
```md
 (Input/Output Per Second)即每秒的输入输出量(或读写次数)，是衡量磁盘性能的主要指标之一。
 随机读写频繁的应用，如OLTP(Online Transaction Processing)，IOPS是关键衡量指标。
```

#### IOPS 细分指标
* Toatal IOPS，混合读写和顺序随机I/O 负载情况下的磁盘 IOPS，这个与实际I/O情况最为相符，大多数应用关注此指标。
* Random Read IOPS，100%随机读负载情况下的IOPS。
* Random Write IOPS，100%随机写负载情况下的IOPS。
* Sequential Read IOPS，100%顺序负载读情况下的IOPS。
* Sequential Write IOPS，100%顺序写负载情况下的IOPS。

#### IOPS 计算
```md
IO Time = Seek Time + 60 sec/Rotational Speed/2 + IO Chunk Size/Transfer Rate //单次IO时间
IOPS = 1/IO Time = 1/(Seek Time + 60 sec/Rotational Speed/2 + IO Chunk Size/Transfer Rate)
```
```md
理论上可以计算出磁盘的最大IOPS，即 IOPS = 1000 ms/ (寻道时间 + 旋转延迟)，忽略数据传输时间。
假设磁盘平均物理寻道时间为3ms, 磁盘转速为7200rpm，则磁盘IOPS理论最大值分别为，
IOPS = 1000 / (3 + 60000/7200/2) = 140
```
```md
固态硬盘SSD是一种电子装置， 避免了传统磁盘在寻道和旋转上的时间花费，存储单元寻址开销大大降低，
因此 IOPS 可以非常高，能够达到数万甚至数十万。
```
```md
实际测量中，IOPS数值会受到很多因素的影响，包括I/O负载特征(读写比例，顺序和随机，工作线程数，队列深度，数据记录大小)、
系统配置、操作系统、磁盘驱动等等。
因此对比测量磁盘IOPS时，必须在同样的测试基准下进行，即便如何也会产生一定的随机不确定性。
```

#### IOPS 统计工具
* Iometer
* IoZone
* FIO

#### IOPS 测试
```md
对于应用系统，需要首先确定数据的负载特征，然后选择合理的IOPS指标进行测量和对比分析，据此选择合适的存储介质和软件系统。
```

### 吞吐量(Throughput)
```md
单位时间内可以成功传输的数据数量。
顺序读写频繁的应用，如视频点播，关注连续读写性能、数据吞吐量是关键衡量指标。
它主要取决于磁盘阵列的架构，通道的大小以及磁盘的个数。
但他们都有自己的内部带宽，一般情况下，内部带宽都设计足够充足，不会存在瓶颈。

磁盘阵列与服务器之间的数据通道对吞吐量影响很大，比如一个2Gbps的光纤通道，其所能支撑的最大流量仅为250MB/s。
最后，当前面的瓶颈都不再存在时，硬盘越多的情况下吞吐量越大。
```

* [/proc/diskstats](../tools/proc/diskstats.md)

* 磁盘IO使用率
```md
采集方式：累计值，每10秒采集一次取差值； 指标: disk.io (key: system)；
io_ticks: 磁盘处理I/O的总时间(ms)，ioticks/1000/10 * 100 -> ioticks/100(0-100的百分比);
```
* 磁盘队列等待时间
```md
数据源: /proc/diskstats; 采集方式：累计值，每10秒采集一次取差值； 指标: disk.io (key: system)；
time_in_queue: 对io_ticks的加权值。
io_ticks是自然时间，不考虑当前有几个I/O，而time_in_queue是用当前的I/O数量in-flight乘以自然时间。
虽然该字段的名称是time_in_queue，但并不真的只是在队列中的时间，其中还包含了硬盘处理I/O的时间。
iostat在计算avgqu-sz时会用到这个字段; time_in_queue/10 -> 即每秒的磁盘队列等待时间；
```
* 写请求数
* 读请求数
```md
采集方式：累计值，每10秒采集一次取差值； 指标: disk.io (key: system)；
write_ios: 写io数 write_ios/10 -> ops/s;
read_ios: 读io数，read_ios/10 -> ops/s;
```
* 写耗时
* 读耗时
```md
采集方式：累计值，每10秒采集一次取差值； 指标: disk.io (key: system)；
write_ticks/write_ios: 平均每次写操作耗时, 总写耗时(ms)/总写io数;
read_ticks/read_ios: 平均每次读操作耗时，总读耗时(ms)/总读io数;
```
* 写扇区次数
* 读扇区次数
```md
采集方式：累计值，每10秒采集一次取差值； 指标: disk.io (key: system)；
write_sectors/10: 每秒写扇区数;
read_sectors/10: 每秒读扇区数;
```
* 写合并数
* 读合并数
```md
采集方式：累计值，每10秒采集一次取差值； 指标: disk.io (key: system)；
write_merges/10: 每秒写合并数;
read_merges/10: 每秒读合并数;
```

## I/O 优化
### 系统参数
* 磁盘请求队列数
* * 查看磁盘的默认请求队列：
```bash
$ cat /sys/block/vda/queue/nr_requests
128
```
* I/O调度算法
```sh
# cat /sys/block/sda/queue/scheduler
```

### 操作系统层的优化
```md
虽然15000rpm的磁盘计算出的理论最大 IOPS 仅为166，但在实际运行环境中，实际磁盘的 IOPS往往能够突破200甚至更高。
这其实就是在系统调用过程中，操作系统进行了一系列的优化。
```
```md
Page Cache 层在内存中缓存了磁盘上的部分数据。当数据的请求到达时，如果在Cache中存在该数据且是最新的，
则直接将数据传递给用户程序，免除了对底层磁盘的操作，提高了性能。
Cache 层也正是磁盘 IOPS 为什么能突破200的主要原因之一。
```

### 基于磁盘I/O特性设计的技巧
```md
1. 按顺序读取数据；这样就不需要寻道和等待。
2. 数据尽可能一次性读写，减少文件碎片
    文件碎片就是一个文件在存储设备上存放时不是连续存放的，
    大部分的文件系统是先到先得的原则，因此一次性写入可以极大的降低文件碎片的情况。
3. 字节对齐
    IO过程中，数据在这些缓冲区内进行复制也要消耗时间和性能的，
    字节对齐后，这些复制操作可视情况去掉（通过open时传入的参数进行修改）
4. 减少不必要的IO同步操作
    一般IO时，操作系统、磁盘控制器等都会对传下来的数据进行优化，比如将多次write合并成一次，调整IO顺序，数据对齐等操作；
    而IO同步时不经过这些优化操作，一旦传下来的数据诸如字节未对齐，多次 IO等会极大影响 IO的性能。
5. 减少不必要的文件开关操作
    大部分的文件系统中，文件的属性数据和文件的真实数据是分开存放的，open一个文件时，操作系统要先找到文件的属性数据将这些数据载入到内存，
    然后根据属性数据再偏移到文件真实数据存放的扇区对数据进行操作；
    close时一般操作系统会强制刷新数据到存储设备，然后再偏移到文件的属性数据存放的扇区更新属性信息，同样也会强制刷新数据。
6. 减少调用IO的频率，能一次写完的数据不要分成多次
    Linux系统分为用户态和系统态（或叫内核态）；一般的操作都是在用户态下进行的，而一些系统调用需要先切换到系统态才能执行，
    切换时需要将当前环境压入到寄存器，操作完成后再从寄存器中读取环境信息来进行恢复；减少IO的调用次数可以减少系统状态切换的开销。
```
#### 采用追加写
```md
在进行系统设计时，良好的读性能和写性能往往不可兼得。
在许多常见的开源系统中都是优先在保证写性能的前提下来优化读性能。
```
* 那么如何设计能让一个系统拥有良好的写性能呢？
```md
一个好的办法就是采用追加写，每次将数据添加到文件。由于完全是顺序的，所以可以具有非常好的写操作性能。
```
```md
缺点：从文件中读一些数据时将会需要更多的时间：需要倒序扫描，直到找到所需要的内容。
```
```md
当然在一些简单的场景下也能够保证读操作的性能：
```
* * 数据是被整体访问，比如 HDFS
```md
HDFS 建立在一次写多次读的模型之上。
在HDFS 中就是采用了追加写并且设计为高数据吞吐量；
高吞吐量必然以高延迟为代价，所以 HDFS 并不适用于对数据访问要求低延迟的场景；
由于采用是的追加写，也并不适用于任意修改文件的场景。
```
```md
HDFS设计为流式访问大文件，使用大数据块并且采用流式数据访问来保证数据被整体访问，
同时最小化硬盘的寻址开销，只需要一次寻址即可，这时寻址时间相比于传输时延可忽略，从而也拥有良好的读性能。
```
```md
HDFS不适合存储小文件，原因之一是由于NameNode内存不足问题，还有就是因为访问大量小文件需要执行大量的寻址操作，
并且需要不断的从一个 datanode 跳到另一个 datanode，这样会大大降低数据访问性能。
```
* * 知道文件明确的偏移量，比如 Kafka
```md
在Kafka中，采用消息追加的方式来写入每个消息，每个消息读写时都会利用Page Cache的预读和后写特性，
同时partition中都使用顺序读写，以此来提高I/O性能。
```
```md
虽然Kafka能够根据偏移量查找到具体的某个消息，但是查找过程是顺序查找，因此如果数据很大的话，查找效率就很低。
所以Kafka中采用了分段和索引的方式来解决查找效率问题。
Kafka把一个patition大文件又分成了多个小文件段，每个小文件段以偏移量命名，通过多个小文件段，
不仅可以使用二分搜索法很快定位消息，同时也容易定期清除或删除已经消费完的文件，减少磁盘占用。
为了进一步提高查找效率，Kafka为每个分段后的数据建立了索引文件，并通过索引文件稀疏存储来降低元数据占用大小。
```
* 在面对更复杂的读场景（比如按key）时，如何来保证读操作的性能呢？
```md
简单的方式是像Kafka那样，将文件数据有序保存，使用二分查找来优化效率；
或者通过建索引的方式来进行优化；
也可以采用hash的方式将数据分割为不同的桶
```
```md
以上的方法都能增加读操作的性能，但是由于在数据上强加了数据结构，又会降低写操作的性能。
比如如果采用索引的方式来优化读操作，那么在更新索引时就需要更新B-tree中的特定部分，这时候的写操作就是随机写。
```
* 有没有一种办法在保证写性能不损失的同时也提供较好的读性能呢？
* * 日志结构的合并树LSM（The Log-Structured Merge-Tree）
```md
是HBase，LevelDB等 NoSQL 数据库的存储引擎。
```
```md
LSM-tree牺牲了部分读性能，以此来换取写入的最大化性能，特别适用于读需求低，会产生大量插入操作的应用环境。
```
#### 文件合并和元数据优化
```md
目前的大多数文件系统，如XFS/Ext4、GFS、HDFS，在元数据管理、缓存管理等实现策略上都侧重大文件。
这些文件系统在面临海量时在性能和存储效率方面都大幅降低，本节来探讨下海量小文件下的系统设计。
```
```md
常见文件系统在海量小文件应用下性能表现不佳的根本原因是磁盘最适合顺序的大文件I/O读写模式，而非常不适合随机的小文件I/O读写模式。
主要原因体现在元数据管理低效和数据布局低效：
```
* 元数据管理低效
```md
由于小文件数据内容较少，因此元数据的访问性能对小文件访问性能影响巨大。
Ext2文件系统中Inode和Data Block分别保存在不同的物理位置上，一次读操作需要至少经过两次的独立访问。
在海量小文件应用下，Inode的频繁访问，使得原本的并发访问转变为了海量的随机访问，大大降低了性能。
另外，大量的小文件会快速耗尽Inode资源，导致磁盘尽管有大量Data Block剩余也无法存储文件，会浪费磁盘空间。
```
* 数据布局低效
```md
Ext2在Inode中使用多级指针来索引数据块。对于大文件，数据块的分配会尽量连续，这样会具有比较好的空间局部性。
但是对于小文件，数据块可能零散分布在磁盘上的不同位置，并且会造成大量的磁盘碎片，不仅造成访问性能下降，还大量浪费了磁盘空间。
数据块一般为1KB、2KB或4KB，对于小于4KB的小文件，Inode 与数据的分开存储破坏了空间局部性，同时也造成了大量的随机I/O。
```
* 其它影响因素
```md
对于海量小文件应用，常见的I/O流程复杂也是造成磁盘性能不佳的原因。
对于小文件，磁盘的读写所占用的时间较少，而用于文件的open()操作占用了绝大部分系统时间，导致磁盘有效服务时间非常低，磁盘性能低下。
```
* 优化思路
```md
1. 针对数据布局低效，采用小文件合并策略，将小文件合并为大文件。
2. 针对元数据管理低效，优化元数据的存储和管理。
```
##### 小文件合并
* 如何寻址这个大文件中的小文件呢？
```md
其实就是利用一个旁路数据库来记录每个小文件在这个大文件中的偏移量和长度等信息。
其实小文件合并的策略本质上就是通过分层的思想来存储元数据。
中控节点存储一级元数据，也就是大文件与底层块的对应关系；
数据节点存放二级元数据，也就是最终的用户文件在这些一级大块中的存储位置对应关系，经过两级寻址来读写数据。
```
* * 淘宝的TFS就采用了小文件合并存储的策略
```md
TFS 中默认 Block大小为64M，每个块中会存储许多不同的小文件，但是这个块只占用一个Inode。
假设一个Block为64M，数量级为1PB。那么NameServer上会有 1 *1024 *1024 * 1024 / 64 = 16.7M 个 Block。
假设每个Block的元数据大小为0.1K，则占用内存不到2G。
在TFS中，文件名中包含了Block ID 和 File ID，通过Block ID 定位到具体的 DataServer上，
然后DataServer会根据本地记录的信息来得到 File ID 所在Block的偏移量，从而读取到正确的文件内容。
```
##### 元数据管理优化
```md
一般来说元数据信息包括名称、文件大小、设备标识符、用户标识符、用户组标识符等等，
在小文件系统中可以对元数据信息进行精简，仅保存足够的信息即可。
元数据精简可以减少元数据通信延时，同时相同容量的Cache能存储更多的元数据，从而提高元数据使用效率。
```
```md
另外可以在文件名中就包含元数据信息，从而减少一个元数据的查询操作。
```
```md
最后针对特别小的一些文件，可以采取元数据和数据并存的策略，将数据直接存储在元数据之中，通过减少一次寻址操作从而大大提高性能。
```
```md
TFS中文件命名就隐含了位置信息等部分元数据，从而减少了一个元数据的查询操作。
在 Rerserfs 中，对于小于1KB的小文件，Rerserfs 可以将数据直接存储在 Inode 中。
```
### Reference
* [硬盘IO那些事](https://www.jianshu.com/p/f9ac7d87f656)

## Tools
* dd - 测试磁盘性能
```bash
$ time dd if=/dev/zero of=test bs=1M count=1000 
```
* iostat
```sh
iostat -d -k 1 10         #查看TPS和吞吐量信息(磁盘读写速度单位为KB)
iostat -d -m 2            #查看TPS和吞吐量信息(磁盘读写速度单位为MB)
iostat -d -x -k 1 10      #查看设备使用率（%util）、响应时间（await） iostat -c 1 10 #查看cpu状态
```


